{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad3f11a-6134-498e-9942-9e309609625f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "901/901 [==============================] - 34s 23ms/step - loss: 1.1193 - accuracy: 0.6591 - val_loss: 0.8641 - val_accuracy: 0.6993\n",
      "Epoch 2/10\n",
      "901/901 [==============================] - 18s 20ms/step - loss: 0.8061 - accuracy: 0.7294 - val_loss: 0.7385 - val_accuracy: 0.7445\n",
      "Epoch 3/10\n",
      "901/901 [==============================] - 18s 20ms/step - loss: 0.7124 - accuracy: 0.7654 - val_loss: 0.6780 - val_accuracy: 0.7758\n",
      "Epoch 4/10\n",
      "901/901 [==============================] - 18s 20ms/step - loss: 0.6757 - accuracy: 0.7743 - val_loss: 0.6535 - val_accuracy: 0.7783\n",
      "Epoch 5/10\n",
      "901/901 [==============================] - 28s 31ms/step - loss: 0.6559 - accuracy: 0.7765 - val_loss: 0.6391 - val_accuracy: 0.7817\n",
      "Epoch 6/10\n",
      "901/901 [==============================] - 22s 25ms/step - loss: 0.6455 - accuracy: 0.7785 - val_loss: 0.6295 - val_accuracy: 0.7834\n",
      "Epoch 7/10\n",
      "569/901 [=================>............] - ETA: 6s - loss: 0.6382 - accuracy: 0.7788"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Flatten, LSTM, SimpleRNN, Input, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import pickle\n",
    "\n",
    "# Step 1: Load Dataset\n",
    "dataset_path = \"UNSW_NB15_training-set.csv\"  # Replace with your dataset path\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "# Select Relevant Parameters for Training\n",
    "selected_columns = [\n",
    "    \"proto\", \"service\", \"sbytes\", \"dbytes\", \"sttl\", \"swin\", \"dwin\", \"rate\", \"smean\", \"dmean\", \"attack_cat\"\n",
    "]\n",
    "data = data[selected_columns]\n",
    "\n",
    "# Step 2: Preprocessing\n",
    "# Handle Missing Values\n",
    "data = data.fillna(\"Unknown\")\n",
    "\n",
    "# Encode Categorical Features\n",
    "label_encoders = {}\n",
    "for col in ['proto', 'service']:\n",
    "    le = LabelEncoder()\n",
    "    data[col] = le.fit_transform(data[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Encode Attack Categories (target labels)\n",
    "attack_cat_encoder = LabelEncoder()\n",
    "data['attack_cat'] = attack_cat_encoder.fit_transform(data['attack_cat'])\n",
    "\n",
    "# Separate Features and Target\n",
    "X = data.drop(['attack_cat'], axis=1)\n",
    "y = data['attack_cat']\n",
    "\n",
    "# Scale Numerical Features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# One-hot encode the target labels for multi-class classification\n",
    "y_encoded = to_categorical(y)\n",
    "\n",
    "# Split into Train and Test Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 3: Define Models for Hybrid Architecture (ANN, CNN, LSTM, RNN)\n",
    "# ANN\n",
    "ann = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(y_encoded.shape[1], activation='softmax')  # Output layer for multi-class classification\n",
    "])\n",
    "\n",
    "# CNN\n",
    "cnn_input = Input(shape=(X_train.shape[1], 1))\n",
    "cnn = Conv1D(64, kernel_size=3, activation='relu')(cnn_input)\n",
    "cnn = Flatten()(cnn)\n",
    "cnn = Dense(y_encoded.shape[1], activation='softmax')(cnn)\n",
    "\n",
    "# LSTM\n",
    "lstm_input = Input(shape=(X_train.shape[1], 1))\n",
    "lstm = LSTM(64, return_sequences=False, activation='relu')(lstm_input)\n",
    "lstm = Dense(y_encoded.shape[1], activation='softmax')(lstm)\n",
    "\n",
    "# RNN\n",
    "rnn_input = Input(shape=(X_train.shape[1], 1))\n",
    "rnn = SimpleRNN(64, return_sequences=False, activation='relu')(rnn_input)\n",
    "rnn = Dense(y_encoded.shape[1], activation='softmax')(rnn)\n",
    "\n",
    "# Combine Outputs\n",
    "combined = concatenate([ann.output, cnn, lstm, rnn])\n",
    "final_output = Dense(y_encoded.shape[1], activation='softmax')(combined)\n",
    "\n",
    "# Define the Stacked Model\n",
    "stacked_model = Model(inputs=[ann.input, cnn_input, lstm_input, rnn_input], outputs=final_output)\n",
    "stacked_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 4: Train the Hybrid Model\n",
    "X_train_reshaped = np.expand_dims(X_train, axis=-1)  # Reshape for CNN, LSTM, RNN inputs\n",
    "X_test_reshaped = np.expand_dims(X_test, axis=-1)\n",
    "\n",
    "stacked_model.fit(\n",
    "    [X_train, X_train_reshaped, X_train_reshaped, X_train_reshaped], \n",
    "    y_train, \n",
    "    epochs=10, \n",
    "    batch_size=64, \n",
    "    validation_data=([X_test, X_test_reshaped, X_test_reshaped, X_test_reshaped], y_test)\n",
    ")\n",
    "\n",
    "# Step 5: Save the Model and Preprocessing Objects\n",
    "stacked_model.save('hybrid_aclr_model.h5')\n",
    "\n",
    "with open('scaler.pkl', 'wb') as scaler_file:\n",
    "    pickle.dump(scaler, scaler_file)\n",
    "\n",
    "for col, le in label_encoders.items():\n",
    "    with open(f'{col}_encoder.pkl', 'wb') as encoder_file:\n",
    "        pickle.dump(le, encoder_file)\n",
    "\n",
    "# Save attack category encoder\n",
    "with open('attack_cat_encoder.pkl', 'wb') as attack_cat_file:\n",
    "    pickle.dump(attack_cat_encoder, attack_cat_file)\n",
    "\n",
    "print(\"Model and preprocessing objects saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f1941e5-13f2-4346-b87b-2a274f926cd8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 65\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Example Test Input\u001b[39;00m\n\u001b[0;32m     52\u001b[0m manual_test \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproto\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtcp\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mservice\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdmean\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m40\u001b[39m,\n\u001b[0;32m     63\u001b[0m }\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted Botnet Attack Type:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mpredict_attack_type_from_user_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmanual_test\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[7], line 38\u001b[0m, in \u001b[0;36mpredict_attack_type_from_user_input\u001b[1;34m(user_input)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_attack_type_from_user_input\u001b[39m(user_input):\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;66;03m# Load the trained model, encoders, and scaler\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m     model, scaler, encoders, attack_cat_encoder \u001b[38;5;241m=\u001b[39m \u001b[43mload_trained_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# Preprocess the user input\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     input_scaled, input_reshaped \u001b[38;5;241m=\u001b[39m preprocess_user_input(user_input, encoders, scaler)\n",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m, in \u001b[0;36mload_trained_objects\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_trained_objects\u001b[39m():\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# Load the trained hybrid model\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhybrid_aclr_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Load the label encoders and scaler\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaler.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m scaler_file:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Function to load the model, encoders, and scaler\n",
    "def load_trained_objects():\n",
    "    # Load the trained hybrid model\n",
    "    model = load_model('hybrid_aclr_model.h5')\n",
    "    \n",
    "    # Load the label encoders and scaler\n",
    "    with open('scaler.pkl', 'rb') as scaler_file:\n",
    "        scaler = pickle.load(scaler_file)\n",
    "    \n",
    "    encoders = {}\n",
    "    for col in ['proto', 'service']:\n",
    "        with open(f'{col}_encoder.pkl', 'rb') as encoder_file:\n",
    "            encoders[col] = pickle.load(encoder_file)\n",
    "    \n",
    "    # Load attack categories encoder\n",
    "    with open('attack_cat_encoder.pkl', 'rb') as attack_cat_file:\n",
    "        attack_cat_encoder = pickle.load(attack_cat_file)\n",
    "\n",
    "    return model, scaler, encoders, attack_cat_encoder\n",
    "\n",
    "# Function to preprocess user input\n",
    "def preprocess_user_input(user_input, encoders, scaler):\n",
    "    # Encode categorical features\n",
    "    for col in ['proto', 'service']:\n",
    "        user_input[col] = encoders[col].transform([user_input[col]])[0]\n",
    "\n",
    "    # Scale the numerical features\n",
    "    input_scaled = scaler.transform(pd.DataFrame([user_input]))\n",
    "\n",
    "    # Reshape for model input\n",
    "    input_reshaped = np.expand_dims(input_scaled, axis=-1)\n",
    "\n",
    "    return input_scaled, input_reshaped\n",
    "\n",
    "# Function to predict attack type from user input\n",
    "def predict_attack_type_from_user_input(user_input):\n",
    "    # Load the trained model, encoders, and scaler\n",
    "    model, scaler, encoders, attack_cat_encoder = load_trained_objects()\n",
    "\n",
    "    # Preprocess the user input\n",
    "    input_scaled, input_reshaped = preprocess_user_input(user_input, encoders, scaler)\n",
    "\n",
    "    # Predict the attack category\n",
    "    prediction = model.predict([input_scaled, input_reshaped, input_reshaped, input_reshaped])\n",
    "\n",
    "    # Convert prediction to attack category label\n",
    "    attack_category = attack_cat_encoder.inverse_transform([np.argmax(prediction, axis=1)])[0]\n",
    "\n",
    "    return attack_category\n",
    "\n",
    "# Example Test Input\n",
    "manual_test = {\n",
    "    'proto': 'tcp',\n",
    "    'service': 'http',\n",
    "    'sbytes': 4000,\n",
    "    'dbytes': 3000,\n",
    "    'sttl': 64,\n",
    "    'swin': 1024,\n",
    "    'dwin': 512,\n",
    "    'rate': 0.5,\n",
    "    'smean': 50,\n",
    "    'dmean': 40,\n",
    "}\n",
    "\n",
    "print(\"Predicted Botnet Attack Type:\", predict_attack_type_from_user_input(manual_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2a0964-fc59-4fa2-b52c-3792c540a2f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
