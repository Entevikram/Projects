{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5e5416d-a55f-44bd-a5b4-199421017ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "901/901 [==============================] - 18s 15ms/step - loss: 1.1098 - accuracy: 0.6741 - val_loss: 0.8258 - val_accuracy: 0.7255\n",
      "Epoch 2/10\n",
      "901/901 [==============================] - 12s 13ms/step - loss: 0.7623 - accuracy: 0.7478 - val_loss: 0.6990 - val_accuracy: 0.7596\n",
      "Epoch 3/10\n",
      "901/901 [==============================] - 12s 14ms/step - loss: 0.6813 - accuracy: 0.7708 - val_loss: 0.6498 - val_accuracy: 0.7736\n",
      "Epoch 4/10\n",
      "901/901 [==============================] - 12s 14ms/step - loss: 0.6468 - accuracy: 0.7727 - val_loss: 0.6233 - val_accuracy: 0.7755\n",
      "Epoch 5/10\n",
      "901/901 [==============================] - 12s 14ms/step - loss: 0.6269 - accuracy: 0.7743 - val_loss: 0.6096 - val_accuracy: 0.7765\n",
      "Epoch 6/10\n",
      "901/901 [==============================] - 12s 14ms/step - loss: 0.6151 - accuracy: 0.7777 - val_loss: 0.5973 - val_accuracy: 0.7774\n",
      "Epoch 7/10\n",
      "901/901 [==============================] - 13s 14ms/step - loss: 0.6029 - accuracy: 0.7871 - val_loss: 0.5858 - val_accuracy: 0.7948\n",
      "Epoch 8/10\n",
      "901/901 [==============================] - 13s 14ms/step - loss: 0.5904 - accuracy: 0.7936 - val_loss: 0.5743 - val_accuracy: 0.7981\n",
      "Epoch 9/10\n",
      "901/901 [==============================] - 12s 14ms/step - loss: 0.5804 - accuracy: 0.7945 - val_loss: 0.5628 - val_accuracy: 0.7995\n",
      "Epoch 10/10\n",
      "901/901 [==============================] - 12s 14ms/step - loss: 0.5736 - accuracy: 0.7953 - val_loss: 0.5600 - val_accuracy: 0.8004\n",
      "Model and preprocessing objects saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Flatten, LSTM, SimpleRNN, Input, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import pickle\n",
    "\n",
    "# Step 1: Load Dataset\n",
    "dataset_path = \"UNSW_NB15_training-set.csv\"  # Replace with your dataset path\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "# Select Relevant Parameters for Training\n",
    "selected_columns = [\n",
    "    \"proto\", \"service\", \"sbytes\", \"dbytes\", \"sttl\", \"swin\", \"dwin\", \"rate\", \"smean\", \"dmean\", \"attack_cat\"\n",
    "]\n",
    "data = data[selected_columns]\n",
    "\n",
    "# Step 2: Preprocessing\n",
    "# Handle Missing Values\n",
    "data = data.fillna(\"Unknown\")\n",
    "\n",
    "# Encode Categorical Features\n",
    "label_encoders = {}\n",
    "for col in ['proto', 'service']:\n",
    "    le = LabelEncoder()\n",
    "    data[col] = le.fit_transform(data[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Encode Attack Categories (target labels)\n",
    "attack_cat_encoder = LabelEncoder()\n",
    "data['attack_cat'] = attack_cat_encoder.fit_transform(data['attack_cat'])\n",
    "\n",
    "# Separate Features and Target\n",
    "X = data.drop(['attack_cat'], axis=1)\n",
    "y = data['attack_cat']\n",
    "\n",
    "# Scale Numerical Features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# One-hot encode the target labels for multi-class classification\n",
    "y_encoded = to_categorical(y)\n",
    "\n",
    "# Split into Train and Test Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 3: Define Models for Hybrid Architecture (ANN, CNN, LSTM, RNN)\n",
    "# ANN\n",
    "ann = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(y_encoded.shape[1], activation='softmax')  # Output layer for multi-class classification\n",
    "])\n",
    "\n",
    "# CNN\n",
    "cnn_input = Input(shape=(X_train.shape[1], 1))\n",
    "cnn = Conv1D(64, kernel_size=3, activation='relu')(cnn_input)\n",
    "cnn = Flatten()(cnn)\n",
    "cnn = Dense(y_encoded.shape[1], activation='softmax')(cnn)\n",
    "\n",
    "# LSTM\n",
    "lstm_input = Input(shape=(X_train.shape[1], 1))\n",
    "lstm = LSTM(64, return_sequences=False, activation='relu')(lstm_input)\n",
    "lstm = Dense(y_encoded.shape[1], activation='softmax')(lstm)\n",
    "\n",
    "# RNN\n",
    "rnn_input = Input(shape=(X_train.shape[1], 1))\n",
    "rnn = SimpleRNN(64, return_sequences=False, activation='relu')(rnn_input)\n",
    "rnn = Dense(y_encoded.shape[1], activation='softmax')(rnn)\n",
    "\n",
    "# Combine Outputs\n",
    "combined = concatenate([ann.output, cnn, lstm, rnn])\n",
    "final_output = Dense(y_encoded.shape[1], activation='softmax')(combined)\n",
    "\n",
    "# Define the Stacked Model\n",
    "stacked_model = Model(inputs=[ann.input, cnn_input, lstm_input, rnn_input], outputs=final_output)\n",
    "stacked_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 4: Train the Hybrid Model\n",
    "X_train_reshaped = np.expand_dims(X_train, axis=-1)  # Reshape for CNN, LSTM, RNN inputs\n",
    "X_test_reshaped = np.expand_dims(X_test, axis=-1)\n",
    "\n",
    "stacked_model.fit(\n",
    "    [X_train, X_train_reshaped, X_train_reshaped, X_train_reshaped], \n",
    "    y_train, \n",
    "    epochs=10, \n",
    "    batch_size=64, \n",
    "    validation_data=([X_test, X_test_reshaped, X_test_reshaped, X_test_reshaped], y_test)\n",
    ")\n",
    "\n",
    "# Step 5: Save the Model and Preprocessing Objects\n",
    "stacked_model.save('hybrid_aclr_model.h5')\n",
    "\n",
    "with open('scaler.pkl', 'wb') as scaler_file:\n",
    "    pickle.dump(scaler, scaler_file)\n",
    "\n",
    "for col, le in label_encoders.items():\n",
    "    with open(f'{col}_encoder.pkl', 'wb') as encoder_file:\n",
    "        pickle.dump(le, encoder_file)\n",
    "\n",
    "# Save attack category encoder\n",
    "with open('attack_cat_encoder.pkl', 'wb') as attack_cat_file:\n",
    "    pickle.dump(attack_cat_encoder, attack_cat_file)\n",
    "\n",
    "print(\"Model and preprocessing objects saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adb77af2-e513-42f1-b464-e1fae7347c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000012869B01120> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 431ms/step\n",
      "Predicted Botnet Attack Type: Normal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:153: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# Function to load the model, encoders, and scaler\n",
    "def load_trained_objects():\n",
    "    # Load the trained hybrid model\n",
    "    model = load_model('hybrid_aclr_model.h5')\n",
    "    \n",
    "    # Load the label encoders and scaler\n",
    "    with open('scaler.pkl', 'rb') as scaler_file:\n",
    "        scaler = pickle.load(scaler_file)\n",
    "    \n",
    "    encoders = {}\n",
    "    for col in ['proto', 'service']:\n",
    "        with open(f'{col}_encoder.pkl', 'rb') as encoder_file:\n",
    "            encoders[col] = pickle.load(encoder_file)\n",
    "    \n",
    "    # Load attack categories encoder\n",
    "    with open('attack_cat_encoder.pkl', 'rb') as attack_cat_file:\n",
    "        attack_cat_encoder = pickle.load(attack_cat_file)\n",
    "\n",
    "    return model, scaler, encoders, attack_cat_encoder\n",
    "\n",
    "# Function to preprocess user input\n",
    "def preprocess_user_input(user_input, encoders, scaler):\n",
    "    # Encode categorical features\n",
    "    for col in ['proto', 'service']:\n",
    "        user_input[col] = encoders[col].transform([user_input[col]])[0]\n",
    "\n",
    "    # Scale the numerical features\n",
    "    input_scaled = scaler.transform(pd.DataFrame([user_input]))\n",
    "\n",
    "    # Reshape for model input\n",
    "    input_reshaped = np.expand_dims(input_scaled, axis=-1)\n",
    "\n",
    "    return input_scaled, input_reshaped\n",
    "\n",
    "# Function to predict attack type from user input\n",
    "def predict_attack_type_from_user_input(user_input):\n",
    "    # Load the trained model, encoders, and scaler\n",
    "    model, scaler, encoders, attack_cat_encoder = load_trained_objects()\n",
    "\n",
    "    # Preprocess the user input\n",
    "    input_scaled, input_reshaped = preprocess_user_input(user_input, encoders, scaler)\n",
    "\n",
    "    # Predict the attack category\n",
    "    prediction = model.predict([input_scaled, input_reshaped, input_reshaped, input_reshaped])\n",
    "\n",
    "    # Convert prediction to attack category label\n",
    "    attack_category = attack_cat_encoder.inverse_transform([np.argmax(prediction, axis=1)])[0]\n",
    "\n",
    "    return attack_category\n",
    "\n",
    "# Example Test Input\n",
    "manual_test = {\n",
    "    'proto': 'tcp',\n",
    "    'service': 'http',\n",
    "    'sbytes': 4000,\n",
    "    'dbytes': 3000,\n",
    "    'sttl': 64,\n",
    "    'swin': 1024,\n",
    "    'dwin': 512,\n",
    "    'rate': 0.5,\n",
    "    'smean': 50,\n",
    "    'dmean': 40,\n",
    "}\n",
    "\n",
    "print(\"Predicted Botnet Attack Type:\", predict_attack_type_from_user_input(manual_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fac727-4dcb-431b-af35-c836e6300c3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
