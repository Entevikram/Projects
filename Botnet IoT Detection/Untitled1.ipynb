{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac2a0964-fc59-4fa2-b52c-3792c540a2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Step 1: Load Dataset\n",
    "dataset_path = \"UNSW_NB15_training-set.csv\"  # Replace with your dataset path\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "# Select Relevant Parameters for Training\n",
    "selected_columns = [\n",
    "    \"proto\", \"service\", \"sbytes\", \"dbytes\", \"sttl\", \"swin\", \"dwin\", \"rate\", \"smean\", \"dmean\", \"attack_cat\"\n",
    "]\n",
    "data = data[selected_columns]\n",
    "\n",
    "# Step 2: Preprocessing\n",
    "# Handle Missing Values\n",
    "data = data.fillna(\"Unknown\")\n",
    "\n",
    "# Encode Categorical Features\n",
    "label_encoders = {}\n",
    "for col in ['proto', 'service']:\n",
    "    le = LabelEncoder()\n",
    "    data[col] = le.fit_transform(data[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Encode Attack Categories (target labels)\n",
    "attack_cat_encoder = LabelEncoder()\n",
    "data['attack_cat'] = attack_cat_encoder.fit_transform(data['attack_cat'])\n",
    "\n",
    "# Separate Features and Target\n",
    "X = data.drop(['attack_cat'], axis=1)\n",
    "y = data['attack_cat']\n",
    "\n",
    "# Scale Numerical Features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# One-hot encode the target labels for multi-class classification\n",
    "y_encoded = to_categorical(y)\n",
    "\n",
    "# Split into Train and Test Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8437b8df-a7b9-4f52-9987-2d619246f209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "901/901 [==============================] - 19s 16ms/step - loss: 1.0472 - accuracy: 0.6733 - val_loss: 0.8435 - val_accuracy: 0.7013\n",
      "Epoch 2/10\n",
      "901/901 [==============================] - 13s 14ms/step - loss: 0.7981 - accuracy: 0.7329 - val_loss: 0.7480 - val_accuracy: 0.7676\n",
      "Epoch 3/10\n",
      "901/901 [==============================] - 13s 15ms/step - loss: 0.7253 - accuracy: 0.7645 - val_loss: 0.7299 - val_accuracy: 0.7708\n",
      "Epoch 4/10\n",
      "901/901 [==============================] - 12s 13ms/step - loss: 0.6880 - accuracy: 0.7758 - val_loss: 0.6616 - val_accuracy: 0.7743\n",
      "Epoch 5/10\n",
      "901/901 [==============================] - 12s 13ms/step - loss: 0.6574 - accuracy: 0.7778 - val_loss: 0.6406 - val_accuracy: 0.7819\n",
      "Epoch 6/10\n",
      "901/901 [==============================] - 12s 13ms/step - loss: 0.6436 - accuracy: 0.7797 - val_loss: 0.6244 - val_accuracy: 0.7831\n",
      "Epoch 7/10\n",
      "901/901 [==============================] - 12s 13ms/step - loss: 0.6348 - accuracy: 0.7803 - val_loss: 0.6195 - val_accuracy: 0.7850\n",
      "Epoch 8/10\n",
      "901/901 [==============================] - 12s 13ms/step - loss: 0.6285 - accuracy: 0.7816 - val_loss: 0.6168 - val_accuracy: 0.7922\n",
      "Epoch 9/10\n",
      "901/901 [==============================] - 12s 13ms/step - loss: 0.6243 - accuracy: 0.7816 - val_loss: 0.6091 - val_accuracy: 0.7904\n",
      "Epoch 10/10\n",
      "901/901 [==============================] - 12s 13ms/step - loss: 0.6177 - accuracy: 0.7884 - val_loss: 0.5972 - val_accuracy: 0.7993\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1f20402a790>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Flatten, LSTM, SimpleRNN, Input, concatenate\n",
    "\n",
    "# ANN\n",
    "ann = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(y_encoded.shape[1], activation='softmax')  # Output layer for multi-class classification\n",
    "])\n",
    "\n",
    "# CNN\n",
    "cnn_input = Input(shape=(X_train.shape[1], 1))\n",
    "cnn = Conv1D(64, kernel_size=3, activation='relu')(cnn_input)\n",
    "cnn = Flatten()(cnn)\n",
    "cnn = Dense(y_encoded.shape[1], activation='softmax')(cnn)\n",
    "\n",
    "# LSTM\n",
    "lstm_input = Input(shape=(X_train.shape[1], 1))\n",
    "lstm = LSTM(64, return_sequences=False, activation='relu')(lstm_input)\n",
    "lstm = Dense(y_encoded.shape[1], activation='softmax')(lstm)\n",
    "\n",
    "# RNN\n",
    "rnn_input = Input(shape=(X_train.shape[1], 1))\n",
    "rnn = SimpleRNN(64, return_sequences=False, activation='relu')(rnn_input)\n",
    "rnn = Dense(y_encoded.shape[1], activation='softmax')(rnn)\n",
    "\n",
    "# Combine Outputs\n",
    "combined = concatenate([ann.output, cnn, lstm, rnn])\n",
    "final_output = Dense(y_encoded.shape[1], activation='softmax')(combined)\n",
    "\n",
    "# Define the Stacked Model\n",
    "stacked_model = Model(inputs=[ann.input, cnn_input, lstm_input, rnn_input], outputs=final_output)\n",
    "stacked_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Reshape Data for CNN, LSTM, and RNN\n",
    "X_train_reshaped = np.expand_dims(X_train, axis=-1)  # Reshape for CNN, LSTM, RNN inputs\n",
    "X_test_reshaped = np.expand_dims(X_test, axis=-1)\n",
    "\n",
    "# Train the Hybrid Model\n",
    "stacked_model.fit(\n",
    "    [X_train, X_train_reshaped, X_train_reshaped, X_train_reshaped],\n",
    "    y_train,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    validation_data=([X_test, X_test_reshaped, X_test_reshaped, X_test_reshaped], y_test)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6df1d497-05eb-4200-8f27-9b025331e422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and preprocessing objects saved successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the Model\n",
    "stacked_model.save('hybrid_aclr_model.h5')\n",
    "\n",
    "# Save the Scaler\n",
    "with open('scaler.pkl', 'wb') as scaler_file:\n",
    "    pickle.dump(scaler, scaler_file)\n",
    "\n",
    "# Save the Label Encoders\n",
    "for col, le in label_encoders.items():\n",
    "    with open(f'{col}_encoder.pkl', 'wb') as encoder_file:\n",
    "        pickle.dump(le, encoder_file)\n",
    "\n",
    "# Save the Attack Category Encoder\n",
    "with open('attack_cat_encoder.pkl', 'wb') as attack_cat_file:\n",
    "    pickle.dump(attack_cat_encoder, attack_cat_file)\n",
    "\n",
    "print(\"Model and preprocessing objects saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2574899b-f390-49a0-a5ea-38ceebde735c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 576ms/step\n",
      "Predicted Botnet Attack Type: Exploits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:153: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Function to Load the Model and Preprocessing Objects\n",
    "def load_trained_objects():\n",
    "    # Load the trained hybrid model\n",
    "    model = load_model('hybrid_aclr_model.h5')\n",
    "    \n",
    "    # Load the scaler\n",
    "    with open('scaler.pkl', 'rb') as scaler_file:\n",
    "        scaler = pickle.load(scaler_file)\n",
    "    \n",
    "    # Load the label encoders\n",
    "    encoders = {}\n",
    "    for col in ['proto', 'service']:\n",
    "        with open(f'{col}_encoder.pkl', 'rb') as encoder_file:\n",
    "            encoders[col] = pickle.load(encoder_file)\n",
    "    \n",
    "    # Load the attack category encoder\n",
    "    with open('attack_cat_encoder.pkl', 'rb') as attack_cat_file:\n",
    "        attack_cat_encoder = pickle.load(attack_cat_file)\n",
    "\n",
    "    return model, scaler, encoders, attack_cat_encoder\n",
    "\n",
    "# Function to Preprocess User Input\n",
    "def preprocess_user_input(user_input, encoders, scaler):\n",
    "    # Encode categorical features\n",
    "    for col in ['proto', 'service']:\n",
    "        user_input[col] = encoders[col].transform([user_input[col]])[0]\n",
    "\n",
    "    # Scale numerical features\n",
    "    input_scaled = scaler.transform(pd.DataFrame([user_input]))\n",
    "\n",
    "    # Reshape for model input\n",
    "    input_reshaped = np.expand_dims(input_scaled, axis=-1)\n",
    "\n",
    "    return input_scaled, input_reshaped\n",
    "\n",
    "# Function to Predict Attack Type from User Input\n",
    "def predict_attack_type_from_user_input(user_input):\n",
    "    # Load the trained objects\n",
    "    model, scaler, encoders, attack_cat_encoder = load_trained_objects()\n",
    "\n",
    "    # Preprocess the user input\n",
    "    input_scaled, input_reshaped = preprocess_user_input(user_input, encoders, scaler)\n",
    "\n",
    "    # Predict the attack category\n",
    "    prediction = model.predict([input_scaled, input_reshaped, input_reshaped, input_reshaped])\n",
    "    attack_category = attack_cat_encoder.inverse_transform([np.argmax(prediction, axis=1)])[0]\n",
    "\n",
    "    return attack_category\n",
    "\n",
    "# Example Manual Input\n",
    "manual_test = {\n",
    "    'proto': 'tcp',\n",
    "    'service': 'http',\n",
    "    'sbytes': 4000,\n",
    "    'dbytes': 3000,\n",
    "    'sttl': 64,\n",
    "    'swin': 1024,\n",
    "    'dwin': 512,\n",
    "    'rate': 0.5,\n",
    "    'smean': 50,\n",
    "    'dmean': 40,\n",
    "}\n",
    "\n",
    "# Predict the Attack Type\n",
    "predicted_attack = predict_attack_type_from_user_input(manual_test)\n",
    "print(f\"Predicted Botnet Attack Type: {predicted_attack}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703c8bf3-caab-451c-8344-20d02221c604",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
